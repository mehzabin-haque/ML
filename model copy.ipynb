{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from gensim.models import FastText\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67378, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hate\n",
       "0    36583\n",
       "1    30795\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hate'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(dataset, stratify_column, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: pd.DataFrame, the input dataset.\n",
    "    - stratify_column: str, the column used for stratified splitting.\n",
    "    - train_ratio: float, the proportion of the dataset for training (default 0.7).\n",
    "    - val_ratio: float, the proportion of the dataset for validation (default 0.2).\n",
    "    - test_ratio: float, the proportion of the dataset for testing (default 0.1).\n",
    "    - random_state: int, random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - train_data, val_data, test_data: pd.DataFrames, the split datasets.\n",
    "    \"\"\"\n",
    "    # Ensure the split ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5, \"Ratios must sum to 1.\"\n",
    "\n",
    "    # Initial train + validation split\n",
    "    train_data, temp_data = train_test_split(\n",
    "        dataset,\n",
    "        test_size=(val_ratio + test_ratio),\n",
    "        random_state=random_state,\n",
    "        stratify=dataset[stratify_column] if stratify_column else None\n",
    "    )\n",
    "\n",
    "    # Split the remaining data into validation and test sets\n",
    "    val_data, test_data = train_test_split(\n",
    "        temp_data,\n",
    "        test_size=test_ratio / (val_ratio + test_ratio),\n",
    "        random_state=random_state,\n",
    "        stratify=temp_data[stratify_column] if stratify_column else None\n",
    "    )\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shazzad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "bengali_stopwords = set(stopwords.words('bengali'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Cleans text by removing unnecessary characters and symbols.\"\"\"\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep Bengali characters and whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text, tokenizer):\n",
    "    \"\"\"Tokenizes text using Bangla BERT tokenizer.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Removes Bengali stopwords from tokens.\"\"\"\n",
    "    return [token for token in tokens if token not in bengali_stopwords]\n",
    "\n",
    "def preprocess_text(text, tokenizer):\n",
    "    \"\"\"Combines cleaning, tokenization, and stopword removal.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    tokens = tokenize_text(text, tokenizer)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_data(df, stratify_column='hate')\n",
    "\n",
    "train_data['processed_sentence'] = train_data['sentence'].apply(lambda x: preprocess_text(x, bert_tokenizer))\n",
    "val_data['processed_sentence'] = val_data['sentence'].apply(lambda x: preprocess_text(x, bert_tokenizer))\n",
    "test_data['processed_sentence'] = test_data['sentence'].apply(lambda x: preprocess_text(x, bert_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_sentence'] = df['sentence'].apply(lambda x: preprocess_text(x, bert_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fasttext(corpus, embedding_dim=100, window=5, min_count=1, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a FastText embedding model using gensim.\n",
    "\n",
    "    Parameters:\n",
    "    - corpus: list of lists, tokenized sentences.\n",
    "    - embedding_dim: int, size of word vectors.\n",
    "    - window: int, context window size.\n",
    "    - min_count: int, minimum word frequency for inclusion.\n",
    "    - sg: int, skip-gram (1) or CBOW (0).\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained FastText model.\n",
    "    \"\"\"\n",
    "    model = FastText(\n",
    "        sentences=corpus, \n",
    "        vector_size=embedding_dim, \n",
    "        window=window, \n",
    "        min_count=min_count, \n",
    "        sg=sg\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_data['processed_sentence'].tolist()\n",
    "fasttext_model = train_fasttext(corpus)\n",
    "\n",
    "fasttext_model.save(\"fasttext_embeddings.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence_tokens, model):\n",
    "    \"\"\"\n",
    "    Generates sentence embeddings by averaging word embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence_tokens: list, tokens in a sentence.\n",
    "    - model: FastText model.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Sentence embedding vector.\n",
    "    \"\"\"\n",
    "    embeddings = [model.wv[word] for word in sentence_tokens if word in model.wv]\n",
    "    if len(embeddings) == 0:  # Handle case with no valid tokens\n",
    "        return np.zeros(model.vector_size)\n",
    "    return sum(embeddings) / len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fasttext_embedding'] = df['processed_sentence'].apply(lambda tokens: sentence_embedding(tokens, fasttext_model))\n",
    "train_data['fasttext_embedding'] = train_data['processed_sentence'].apply(lambda x: sentence_embedding(x, fasttext_model))\n",
    "val_data['fasttext_embedding'] = val_data['processed_sentence'].apply(lambda x: sentence_embedding(x, fasttext_model))\n",
    "test_data['fasttext_embedding'] = test_data['processed_sentence'].apply(lambda x: sentence_embedding(x, fasttext_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-0.2294447, -0.0554792, 0.43374178, 0.0877842...\n",
       "1    [-0.09326634, -0.058718935, 0.25895825, 0.0064...\n",
       "2    [-0.15248016, -0.1341862, 0.38996747, 0.208053...\n",
       "3    [-0.24937002, -0.071896076, 0.29962087, 0.0268...\n",
       "4    [-0.1607902, 0.07257928, 0.41274455, 0.0196789...\n",
       "Name: fasttext_embedding, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fasttext_embedding'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [##ত, ##সব, পাপন, শালা, ##র, ফাজলাম, ##ী]\n",
       "1      [পাপন, শালা, রে, রিমান, ##ডে, নেও, ##যা, দরকার]\n",
       "2    [জিল, ##ল, ##র, রহমান, স, ##যার, ##ের, ছেলে, #...\n",
       "3             [শালা, ল, ##চ, ##চা, পাঠ, ##ার, মত, ##য]\n",
       "4    [ত, ##ই, তে, ##া, শালা, গাজা, খাই, ##ছ, ##চতর,...\n",
       "Name: processed_sentence, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_sentence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_sentence_embedding(text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    Returns:\n",
    "        torch.Tensor of shape [bert_hidden_size]\n",
    "    \"\"\"\n",
    "    inputs = bert_tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    # Inference with no gradient to save memory and compute\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # outputs.last_hidden_state has shape [batch_size, seq_len, hidden_dim]\n",
    "    # If you're only handling a single text, you can squeeze the batch dimension\n",
    "    return outputs.last_hidden_state.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_embeddings(tokens, fasttext_model, bert_embeddings, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tokens (list): List of tokens\n",
    "        fasttext_model: Gensim's FastText model\n",
    "        bert_embeddings (torch.Tensor): BERT embeddings on GPU\n",
    "        embedding_dim (int): Dimension of the FastText embeddings\n",
    "    Returns:\n",
    "        torch.Tensor of shape [len(tokens), embedding_dim + bert_hidden_size]\n",
    "    \"\"\"\n",
    "    combined_embeddings = []\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        # 1. Get FastText vector (on CPU), then move to GPU\n",
    "        if token in fasttext_model.wv:\n",
    "            fasttext_vec = fasttext_model.wv[token]\n",
    "        else:\n",
    "            fasttext_vec = np.zeros(embedding_dim)\n",
    "        fasttext_vec = torch.tensor(fasttext_vec, dtype=torch.float32, device=device)\n",
    "\n",
    "        # 2. Get the corresponding BERT embedding (already on GPU)\n",
    "        if idx < bert_embeddings.size(0):\n",
    "            bert_vec = bert_embeddings[idx]\n",
    "        else:\n",
    "            bert_vec = torch.zeros_like(bert_embeddings[0], device=device)\n",
    "\n",
    "        # 3. Concatenate FastText and BERT embedding along the last dimension\n",
    "        combined_vec = torch.cat((fasttext_vec, bert_vec), dim=0)\n",
    "        combined_embeddings.append(combined_vec)\n",
    "\n",
    "    # Convert list of tensors to a single tensor of shape [num_tokens, total_dim]\n",
    "    combined_embeddings = torch.stack(combined_embeddings)\n",
    "    return combined_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_safe(text, fasttext_model):\n",
    "    \"\"\"\n",
    "    Returns a zero embedding if the text is empty\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        # For example, return a zero tensor with shape [1, your_dimension]\n",
    "        # or skip. This is up to your design.\n",
    "        return torch.zeros((1, 868), device=device)  # If your BERT hidden dim is 768\n",
    "    tokens = text.split()\n",
    "    bert_emb = bert_sentence_embedding(\" \".join(tokens))  # get_bert_embeddings also on GPU\n",
    "    return combine_embeddings(tokens, fasttext_model, bert_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['processed_sentence'].apply(lambda tokens: ' '.join(tokens))\n",
    "y = df['hate']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train embeddings: 100%|██████████| 53902/53902 [08:40<00:00, 103.65it/s]\n",
      "Generating test embeddings: 100%|██████████| 13476/13476 [02:12<00:00, 101.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = [\n",
    "    get_embeddings_safe(tokens, fasttext_model) \n",
    "    for tokens in tqdm(X_train, desc=\"Generating train embeddings\")\n",
    "]\n",
    "test_embeddings = [\n",
    "    get_embeddings_safe(tokens, fasttext_model) \n",
    "    for tokens in tqdm(X_test, desc=\"Generating test embeddings\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx].float(), torch.tensor(self.labels[idx], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_classes, dropout=0.3):\n",
    "        super(HAN, self).__init__()\n",
    "\n",
    "        self.word_lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.word_attention = nn.Linear(2 * hidden_dim, 1)\n",
    "        self.sentence_lstm = nn.LSTM(2 * hidden_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.sentence_attention = nn.Linear(2 * hidden_dim, 1)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, 1)\n",
    "\n",
    "    def attention(self, lstm_output, attention_layer):\n",
    "        \"\"\"\n",
    "        Computes attention scores and applies attention mechanism.\n",
    "\n",
    "        Parameters:\n",
    "        - lstm_output: Tensor [batch_size, seq_len, hidden_dim]\n",
    "        - attention_layer: nn.Linear, attention layer\n",
    "        \n",
    "        Returns:\n",
    "        - weighted_output: Tensor [batch_size, hidden_dim]\n",
    "        \"\"\"\n",
    "        attention_weights = torch.softmax(attention_layer(lstm_output), dim=1)\n",
    "        weighted_output = torch.sum(attention_weights * lstm_output, dim=1)\n",
    "        return weighted_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the HAN model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Tensor [batch_size, num_sentences, num_words, embedding_dim] or [batch_size, num_words, embedding_dim]\n",
    "        \n",
    "        Returns:\n",
    "        - logits: Tensor [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # Preprocess input to ensure it has four dimensions\n",
    "        word_output, _ = self.word_lstm(x)\n",
    "        sentence_input = self.attention(word_output, self.word_attention)\n",
    "        sentence_output, _ = self.sentence_lstm(sentence_input.unsqueeze(1))\n",
    "        document_representation = self.attention(sentence_output, self.sentence_attention)\n",
    "        logits = self.fc(document_representation)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Each item in 'batch' is a tuple: (embeddings, label)\n",
    "    - embeddings: shape [seq_len, embedding_dim]\n",
    "    - label: scalar\n",
    "    We'll pad embeddings so they all match the longest seq_len in this batch.\n",
    "    \"\"\"\n",
    "    embeddings_list = [item[0] for item in batch]  # list of [seq_len, embed_dim] tensors\n",
    "    labels_list = [item[1] for item in batch]      # list of label tensors\n",
    "\n",
    "    # Pad embeddings to [batch_size, max_seq_len_in_batch, embed_dim]\n",
    "    padded_embeddings = pad_sequence(embeddings_list, batch_first=True)\n",
    "\n",
    "    # Stack labels, shape [batch_size]\n",
    "    labels_tensor = torch.stack(labels_list)\n",
    "\n",
    "    return padded_embeddings, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset_han = HateSpeechDataset(train_embeddings, y_train.tolist())\n",
    "test_dataset_han = HateSpeechDataset(test_embeddings, y_test.tolist())\n",
    "train_loader_han = DataLoader(train_dataset_han, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader_han = DataLoader(test_dataset_han, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53902"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset_han.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1685"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil(len(train_loader_han.dataset) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 868  # 100 (FastText) + 768 (Bangla BERT)\n",
    "hidden_dim = 256\n",
    "num_classes = len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Linear.__init__() missing 1 required positional argument: 'out_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mHAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "Cell \u001b[1;32mIn[50], line 12\u001b[0m, in \u001b[0;36mHAN.__init__\u001b[1;34m(self, embedding_dim, hidden_dim, num_classes, dropout)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_lstm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m hidden_dim, hidden_dim, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_attention \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m hidden_dim, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Linear.__init__() missing 1 required positional argument: 'out_features'"
     ]
    }
   ],
   "source": [
    "model = HAN(embedding_dim, hidden_dim, num_classes).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_han(model, \n",
    "             train_loader, \n",
    "             test_loader, \n",
    "             criterion, \n",
    "             optimizer, \n",
    "             epochs, \n",
    "             device, \n",
    "             save_path=None,\n",
    "             early_stopping_patience=None):\n",
    "    \"\"\"\n",
    "    Trains the HAN model and evaluates on the test set after each epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): The HAN model to train.\n",
    "    - train_loader (DataLoader): DataLoader for the training data.\n",
    "    - test_loader (DataLoader): DataLoader for the testing/validation data.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (torch.device): Device to run the model on.\n",
    "    - save_path (str, optional): Path to save the best model. Defaults to None.\n",
    "    - early_stopping_patience (int, optional): Number of epochs with no improvement after which training will be stopped. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_f1 = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} - Training\", leave=False)\n",
    "\n",
    "        for batch_idx, (embeddings, labels) in enumerate(train_bar):\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            train_bar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            eval_bar = tqdm(test_loader, desc=f\"Epoch {epoch}/{epochs} - Evaluating\", leave=False)\n",
    "            for embeddings, labels in eval_bar:\n",
    "                embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "                outputs = model(embeddings)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - \"\n",
    "              f\"Accuracy: {accuracy:.4f} | \"\n",
    "              f\"Precision: {precision:.4f} | \"\n",
    "              f\"Recall: {recall:.4f} | \"\n",
    "              f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            epochs_no_improve = 0\n",
    "            if save_path:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"Best model saved to {save_path}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if early_stopping_patience and epochs_no_improve >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        # if save_path:\n",
    "        #     checkpoint_path = f\"epoch_{epoch}_model.pth\"\n",
    "        #     torch.save(model.state_dict(), checkpoint_path)\n",
    "        #     print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(f\"Training completed. Best F1 Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"best_model.pth\"\n",
    "early_stopping_patience = 5\n",
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32])) must be the same as input size (torch.Size([32, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_han\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m         \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_han\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m         \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader_han\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m         \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m         \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m         \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 46\u001b[0m, in \u001b[0;36mtrain_han\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, epochs, device, save_path, early_stopping_patience)\u001b[0m\n\u001b[0;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(embeddings)\n\u001b[1;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     48\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:819\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\torch\\nn\\functional.py:3624\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m-> 3624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3625\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3626\u001b[0m     )\n\u001b[0;32m   3628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[0;32m   3629\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[0;32m   3630\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([32])) must be the same as input size (torch.Size([32, 1]))"
     ]
    }
   ],
   "source": [
    "train_han(model=model, \n",
    "         train_loader=train_loader_han, \n",
    "         test_loader=test_loader_han, \n",
    "         criterion=criterion, \n",
    "         optimizer=optimizer, \n",
    "         epochs=epochs, \n",
    "         device=device, \n",
    "         save_path=save_path,\n",
    "         early_stopping_patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_filters, filter_sizes, output_dim, dropout):\n",
    "        \"\"\"\n",
    "        Conv-LSTM model for hate speech detection.\n",
    "\n",
    "        Parameters:\n",
    "        - embedding_dim: Dimensionality of input embeddings (FastText + BERT combined).\n",
    "        - hidden_dim: Hidden size of the LSTM layer.\n",
    "        - num_filters: Number of filters for convolutional layers.\n",
    "        - filter_sizes: List of filter sizes (e.g., [2, 3, 4]).\n",
    "        - output_dim: Number of output classes (e.g., 1 for binary classification).\n",
    "        - dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=num_filters, \n",
    "                      kernel_size=(fs, embedding_dim)) \n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.lstm = nn.LSTM(num_filters * len(filter_sizes), hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Conv-LSTM model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Tensor [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        Returns:\n",
    "        - logits: Tensor [batch_size, output_dim]\n",
    "        \"\"\"\n",
    "        # Add a channel dimension for Conv2D\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Apply convolution + ReLU + MaxPooling\n",
    "        conv_outs = [torch.relu(conv(x)) for conv in self.convs]  # [batch_size, num_filters, seq_len - kernel_size + 1]\n",
    "        pooled_outs = [torch.max(conv_out, dim=2)[0] for conv_out in conv_outs]  # [batch_size, num_filters]\n",
    "        concat_pooled = torch.cat(pooled_outs, dim=1)  # [batch_size, num_filters * len(filter_sizes)]\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(concat_pooled.unsqueeze(1))  # [batch_size, seq_len=1, hidden_dim*2]\n",
    "        lstm_out = lstm_out.squeeze(1)  # [batch_size, hidden_dim*2]\n",
    "        \n",
    "        logits = self.fc(lstm_out)  # [batch_size, output_dim]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 100\n",
    "filter_sizes = [2, 3, 4, 5]\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clmodel = ConvLSTM(embedding_dim=embedding_dim, hidden_dim=hidden_dim, num_filters=num_filters, filter_sizes=filter_sizes, output_dim=output_dim, dropout=0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conv_lstm(model, \n",
    "                    train_loader, \n",
    "                    test_loader, \n",
    "                    criterion, \n",
    "                    optimizer,  \n",
    "                    device='cuda', \n",
    "                    epochs=10, \n",
    "                    save_path=None, \n",
    "                    early_stopping_patience=None):\n",
    "    \"\"\"\n",
    "    Trains the Conv-LSTM model and evaluates it on the test set after each epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): The Conv-LSTM model to train.\n",
    "    - train_loader (DataLoader): DataLoader for the training data.\n",
    "    - test_loader (DataLoader): DataLoader for the testing/validation data.\n",
    "    - criterion (nn.Module): Loss function (e.g., BCEWithLogitsLoss or CrossEntropyLoss).\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer (e.g., Adam).\n",
    "    - scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler. Defaults to None.\n",
    "    - device (str): Device to run the model on ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "    - epochs (int): Number of training epochs. Defaults to 10.\n",
    "    - save_path (str, optional): Path to save the best model. Defaults to None.\n",
    "    - early_stopping_patience (int, optional): Early stopping patience. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_f1 = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for embeddings, labels in train_bar:\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(embeddings)  # Forward pass\n",
    "            loss = criterion(outputs.squeeze(1), labels.float())  # Adjust for binary classification\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Gradient clipping\n",
    "            optimizer.step()  # Optimizer step\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            train_bar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            eval_bar = tqdm(test_loader, desc=\"Evaluating\", leave=False)\n",
    "            for embeddings, labels in eval_bar:\n",
    "                embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs.squeeze(1), labels.float())  # Validation loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.sigmoid(outputs.squeeze(1)) >= 0.5  # Binary prediction threshold\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            epochs_no_improve = 0\n",
    "            if save_path:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"Best model saved to {save_path}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if early_stopping_patience and epochs_no_improve >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(f\"Training completed. Best F1 Score: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7028\n",
      "Accuracy: 0.4072 | Precision: 0.6250 | Recall: 0.0034 | F1 Score: 0.0068\n",
      "Best model saved to best_conv_lstm_model.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 2/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7028\n",
      "Accuracy: 0.4072 | Precision: 0.6250 | Recall: 0.0034 | F1 Score: 0.0068\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 3/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7028\n",
      "Accuracy: 0.4072 | Precision: 0.6250 | Recall: 0.0034 | F1 Score: 0.0068\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 4/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7028\n",
      "Accuracy: 0.4072 | Precision: 0.6250 | Recall: 0.0034 | F1 Score: 0.0068\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 5/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7028\n",
      "Accuracy: 0.4072 | Precision: 0.6250 | Recall: 0.0034 | F1 Score: 0.0068\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 6/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7028\n",
      "Accuracy: 0.4072 | Precision: 0.6250 | Recall: 0.0034 | F1 Score: 0.0068\n",
      "Early stopping triggered.\n",
      "Training completed. Best F1 Score: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_conv_lstm(\n",
    "    model=clmodel,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    epochs=20,\n",
    "    save_path=\"best_conv_lstm_model.pth\",\n",
    "    early_stopping_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDatasetBERT(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Custom Dataset for Bangla-BERT Fine-Tuning.\n",
    "        Parameters:\n",
    "        - texts: List of sentences.\n",
    "        - labels: List of binary labels (0 or 1).\n",
    "        - tokenizer: Pre-trained Bangla-BERT tokenizer.\n",
    "        - max_length: Maximum sequence length for padding/truncation.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split your dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = df['sentence'].tolist()\n",
    "labels = df['hate'].tolist()\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = HateSpeechDatasetBERT(train_texts, train_labels, bert_tokenizer)\n",
    "test_dataset_bert = HateSpeechDatasetBERT(test_texts, test_labels, bert_tokenizer)\n",
    "\n",
    "train_loader_bert = DataLoader(train_dataset_bert, batch_size=16, shuffle=True)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=16, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
