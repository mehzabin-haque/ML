{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python\\lib\\site-packages (2.2.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\python\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: nltk in c:\\python\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: transformers in c:\\python\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: fasttext-wheel in c:\\python\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: torch in c:\\python\\lib\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: shap in c:\\python\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: matplotlib in c:\\python\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\python\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shazzad\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click in c:\\python\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\python\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: filelock in c:\\python\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\python\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shazzad\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\python\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\python\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\python\\lib\\site-packages (from fasttext-wheel) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\python\\lib\\site-packages (from fasttext-wheel) (75.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: networkx in c:\\python\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\python\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in c:\\python\\lib\\site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\python\\lib\\site-packages (from shap) (3.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\python\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shazzad\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\python\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\python\\lib\\site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy nltk transformers fasttext-wheel scikit-learn torch shap matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shazzad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the Bengali stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load Bengali stopwords\n",
    "bengali_stopwords = set(stopwords.words('bengali'))\n",
    "\n",
    "# Initialize tokenizer for later use\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preview:\n",
      "                                            sentence  hate category\n",
      "0                     যত্তসব পাপন শালার ফাজলামী!!!!!     1   sports\n",
      "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার     1   sports\n",
      "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...     1   sports\n",
      "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়     1   sports\n",
      "4   তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব     1   sports\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Load the Dataset\n",
    "# Replace 'dataset.csv' with the path to your dataset file\n",
    "file_path = 'bangla_hate_speech.csv'  # Update this with your actual dataset path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to verify data loading\n",
    "print(\"Data Preview:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Cleaning:\n",
      "                                            sentence  \\\n",
      "0                     যত্তসব পাপন শালার ফাজলামী!!!!!   \n",
      "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার   \n",
      "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...   \n",
      "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়   \n",
      "4   তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব   \n",
      "\n",
      "                                    cleaned_sentence  \n",
      "0                          যত্তসব পাপন শালার ফাজলামী  \n",
      "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার  \n",
      "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...  \n",
      "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়  \n",
      "4    তুই তো শালা গাজা খাইছচতুর মার হেডায় খেলবে সাকিব  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 3: Text Cleaning\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing unnecessary characters and symbols.\n",
    "    \"\"\"\n",
    "    # Remove special characters, numbers, and symbols, keeping only Bengali letters\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep Bengali characters and whitespace\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'sentence' column\n",
    "df['cleaned_sentence'] = df['sentence'].apply(clean_text)\n",
    "\n",
    "print(\"After Cleaning:\")\n",
    "print(df[['sentence', 'cleaned_sentence']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Tokenization:\n",
      "                                    cleaned_sentence  \\\n",
      "0                          যত্তসব পাপন শালার ফাজলামী   \n",
      "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার   \n",
      "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...   \n",
      "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়   \n",
      "4    তুই তো শালা গাজা খাইছচতুর মার হেডায় খেলবে সাকিব   \n",
      "\n",
      "                                              tokens  \n",
      "0      [যত, ##ত, ##সব, পাপন, শালা, ##র, ফাজলাম, ##ী]  \n",
      "1    [পাপন, শালা, রে, রিমান, ##ডে, নেও, ##যা, দরকার]  \n",
      "2  [জিল, ##ল, ##র, রহমান, স, ##যার, ##ের, ছেলে, এ...  \n",
      "3  [শালা, ল, ##চ, ##চা, দেখতে, পাঠ, ##ার, মত, দেখ...  \n",
      "4  [ত, ##ই, তে, ##া, শালা, গাজা, খাই, ##ছ, ##চতর,...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 4: Tokenization with Bangla BERT\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text using Bangla BERT tokenizer.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into subwords for better context understanding\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Convert tokens to a single string for training input\n",
    "    return tokens\n",
    "\n",
    "# Tokenize cleaned sentences\n",
    "df['tokens'] = df['cleaned_sentence'].apply(tokenize_text)\n",
    "\n",
    "print(\"After Tokenization:\")\n",
    "print(df[['cleaned_sentence', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stopword Removal:\n",
      "                                              tokens  \\\n",
      "0      [যত, ##ত, ##সব, পাপন, শালা, ##র, ফাজলাম, ##ী]   \n",
      "1    [পাপন, শালা, রে, রিমান, ##ডে, নেও, ##যা, দরকার]   \n",
      "2  [জিল, ##ল, ##র, রহমান, স, ##যার, ##ের, ছেলে, এ...   \n",
      "3  [শালা, ল, ##চ, ##চা, দেখতে, পাঠ, ##ার, মত, দেখ...   \n",
      "4  [ত, ##ই, তে, ##া, শালা, গাজা, খাই, ##ছ, ##চতর,...   \n",
      "\n",
      "                                     filtered_tokens  \n",
      "0          [##ত, ##সব, পাপন, শালা, ##র, ফাজলাম, ##ী]  \n",
      "1    [পাপন, শালা, রে, রিমান, ##ডে, নেও, ##যা, দরকার]  \n",
      "2  [জিল, ##ল, ##র, রহমান, স, ##যার, ##ের, ছেলে, #...  \n",
      "3           [শালা, ল, ##চ, ##চা, পাঠ, ##ার, মত, ##য]  \n",
      "4  [ত, ##ই, তে, ##া, শালা, গাজা, খাই, ##ছ, ##চতর,...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 5: Stopword Removal\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Removes Bengali stopwords from the token list.\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token not in bengali_stopwords]\n",
    "\n",
    "# Apply stopword removal on tokens\n",
    "df['filtered_tokens'] = df['tokens'].apply(remove_stopwords)\n",
    "\n",
    "print(\"After Stopword Removal:\")\n",
    "print(df[['tokens', 'filtered_tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 24000\n",
      "Testing Data Size: 6000\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Prepare Data for Modeling\n",
    "# Map the sentences and their respective labels\n",
    "X = df['filtered_tokens'].apply(lambda tokens: ' '.join(tokens))  # Join tokens for input\n",
    "y = df['hate']  # Assuming the 'hate' column contains the labels\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Display dataset statistics\n",
    "print(\"Training Data Size:\", len(X_train))\n",
    "print(\"Testing Data Size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Data saved to disk.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Save Preprocessed Data\n",
    "# Save the train and test splits for later use\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing complete. Data saved to disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\python\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\python\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\python\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\python\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\python\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from gensim.models import FastText\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText training complete!\n",
      "FastText vector for 'বাংলাদেশ': [-0.3095373  -0.31599662  0.588646    0.269986   -0.19336222 -0.3560959\n",
      "  0.16978024  0.5795253  -0.40246212 -0.42315122  0.25908586  0.35319906\n",
      "  0.08453769 -0.5304741  -0.58906287 -0.10794803  0.06659401 -0.7531366\n",
      "  0.13634633 -0.01830969 -0.44294015  0.49936166 -0.21476266 -0.01546686\n",
      "  0.4388483  -0.37863943 -0.17825086  0.29273772  0.26894897 -0.0129158\n",
      " -0.18571742  0.47350574 -0.20383324  0.38840663  0.07102347 -0.19870542\n",
      " -0.00942242 -0.3821738  -0.09808816 -0.18958327  0.5214082   0.25697157\n",
      " -0.30335066 -0.33748052  0.27202594 -0.2883737  -0.15581124  0.45094603\n",
      " -0.21410267 -0.2759681   0.6139857  -0.19837175  0.5427403  -0.11001705\n",
      " -0.20757355  0.0792667   0.18060471 -0.24376367 -0.48467174  0.32077911\n",
      "  0.2312523  -0.00254642  0.03805562 -0.18511496  0.07985787 -0.53999215\n",
      " -0.03044677 -0.30813614 -0.11011249  0.08694138 -0.2902234   0.24777886\n",
      "  0.5570032   0.10141779  0.49019662 -0.1341573  -0.25346276  0.08647366\n",
      "  0.20682773 -0.20717846  0.26984018  0.6376342   0.10991403  0.34424558\n",
      "  0.110439    0.23404607 -0.22692811  0.02822324 -0.31702688 -0.04220414\n",
      "  0.57713455 -0.05336256 -0.06413148  0.06356031  0.04628435 -0.43480325\n",
      "  0.6119976   0.06970321  0.39300448  0.5111206 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: FastText Embeddings\n",
    "def train_fasttext(corpus, embedding_dim=100, window_size=5, min_count=1):\n",
    "    \"\"\"\n",
    "    Train a FastText model on the dataset.\n",
    "    :param corpus: List of tokenized sentences.\n",
    "    :param embedding_dim: Dimension of the embeddings.\n",
    "    :param window_size: Context window size.\n",
    "    :param min_count: Minimum word count threshold.\n",
    "    :return: Trained FastText model.\n",
    "    \"\"\"\n",
    "    model = FastText(sentences=corpus, vector_size=embedding_dim, window=window_size, min_count=min_count, sg=1)\n",
    "    return model\n",
    "\n",
    "# Convert the filtered tokens to a list of tokenized sentences\n",
    "corpus = df['filtered_tokens'].tolist()\n",
    "\n",
    "# Train the FastText model\n",
    "fasttext_model = train_fasttext(corpus)\n",
    "print(\"FastText training complete!\")\n",
    "\n",
    "# Example: Retrieve FastText vector for a word\n",
    "word = \"বাংলাদেশ\"\n",
    "if word in fasttext_model.wv:\n",
    "    print(f\"FastText vector for '{word}': {fasttext_model.wv[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Embeddings shape: torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load Bangla BERT model and tokenizer\n",
    "bangla_bert_model = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\").to(device)\n",
    "bangla_bert_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    \"\"\"\n",
    "    Extract BERT embeddings for the input text.\n",
    "    :param text: Input sentence.\n",
    "    :return: Token-level embeddings.\n",
    "    \"\"\"\n",
    "    # Tokenize input and move tensors to the same device as the model\n",
    "    inputs = bangla_bert_tokenizer(text, return_tensors=\"pt\", \n",
    "                                   padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bangla_bert_model(**inputs)\n",
    "        \n",
    "    # Get the last hidden state (token embeddings) and move back to CPU if needed\n",
    "    token_embeddings = outputs.last_hidden_state.squeeze(0).cpu()\n",
    "    return token_embeddings\n",
    "\n",
    "# Example: Get Bangla BERT embeddings for a sentence\n",
    "sentence = \"বাংলাদেশ একটি সুন্দর দেশ।\"\n",
    "bert_embeddings = get_bert_embeddings(sentence)\n",
    "print(f\"BERT Embeddings shape: {bert_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Embeddings shape: (7, 868)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 3: Combine FastText and BERT Embeddings\n",
    "def combine_embeddings(tokens, fasttext_model, bert_embeddings, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Combine FastText and BERT embeddings for each token.\n",
    "    :param tokens: List of tokens for the sentence.\n",
    "    :param fasttext_model: Trained FastText model.\n",
    "    :param bert_embeddings: BERT embeddings for the tokens.\n",
    "    :param embedding_dim: Dimension of FastText embeddings.\n",
    "    :return: Combined embeddings for each token.\n",
    "    \"\"\"\n",
    "    combined_embeddings = []\n",
    "    for idx, token in enumerate(tokens):\n",
    "        # Get FastText embedding (zeros if not in vocab)\n",
    "        fasttext_vec = fasttext_model.wv[token] if token in fasttext_model.wv else np.zeros(embedding_dim)\n",
    "        \n",
    "        # Get BERT embedding for the token\n",
    "        bert_vec = bert_embeddings[idx].numpy() if idx < len(bert_embeddings) else np.zeros_like(bert_embeddings[0].numpy())\n",
    "        \n",
    "        # Concatenate FastText and BERT embeddings\n",
    "        combined_vec = np.concatenate((fasttext_vec, bert_vec))\n",
    "        combined_embeddings.append(combined_vec)\n",
    "    \n",
    "    return np.array(combined_embeddings)\n",
    "\n",
    "# Example: Combine embeddings for a sentence\n",
    "tokens = df['filtered_tokens'][0]  # Use the first sentence in the dataset\n",
    "bert_embs = get_bert_embeddings(\" \".join(tokens))\n",
    "combined_embs = combine_embeddings(tokens, fasttext_model, bert_embs)\n",
    "print(f\"Combined Embeddings shape: {combined_embs.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the Custom Dataset\n",
    "class HateSpeechDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading hate speech data with embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, embeddings):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embedding = self.embeddings[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(text_embedding, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Define the HAN Model\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_classes):\n",
    "        super(HAN, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Word-level BiLSTM\n",
    "        self.word_lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.word_attention = nn.Linear(2 * hidden_dim, 1)  # Attention layer for words\n",
    "\n",
    "        # Sentence-level BiLSTM\n",
    "        self.sentence_lstm = nn.LSTM(2 * hidden_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.sentence_attention = nn.Linear(2 * hidden_dim, 1)  # Attention layer for sentences\n",
    "\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(2 * hidden_dim, num_classes)\n",
    "\n",
    "    def attention(self, lstm_output, attention_layer):\n",
    "        \"\"\"\n",
    "        Apply attention mechanism.\n",
    "        \"\"\"\n",
    "        attention_weights = torch.softmax(attention_layer(lstm_output), dim=1)\n",
    "        weighted_output = torch.sum(attention_weights * lstm_output, dim=1)\n",
    "        return weighted_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Word-level BiLSTM and attention\n",
    "        word_output, _ = self.word_lstm(x)\n",
    "        sentence_input = self.attention(word_output, self.word_attention)\n",
    "\n",
    "        # Sentence-level BiLSTM and attention\n",
    "        sentence_output, _ = self.sentence_lstm(sentence_input.unsqueeze(0))\n",
    "        document_representation = self.attention(sentence_output, self.sentence_attention)\n",
    "\n",
    "        # Classification layer\n",
    "        logits = self.fc(document_representation)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 3: Training and Evaluation Functions\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for embeddings, labels in train_loader:\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in val_loader:\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "            outputs = model(embeddings)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Convert labels to numpy arrays for easier manipulation\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Step 1: Combine Embeddings and Labels for Training Dataset\n",
    "train_dataset = [\n",
    "    (torch.tensor(embedding, dtype=torch.float32), torch.tensor(label, dtype=torch.long))\n",
    "    for embedding, label in zip(combined_embs, y_train)\n",
    "]\n",
    "\n",
    "# Step 2: Combine Embeddings and Labels for Testing Dataset\n",
    "test_dataset = [\n",
    "    (torch.tensor(embedding, dtype=torch.float32), torch.tensor(label, dtype=torch.long))\n",
    "    for embedding, label in zip(test_embeddings, y_test)\n",
    "]\n",
    "\n",
    "# Step 3: Create PyTorch DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Print statistics to verify\n",
    "print(f\"Number of training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_train_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 4: Preparing Data and Training the Model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Assuming you have combined embeddings (X_train, X_test) and labels (y_train, y_test)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m HateSpeechDataset(X_train, y_train, \u001b[43mcombined_train_embeddings\u001b[49m)\n\u001b[0;32m      4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m HateSpeechDataset(X_test, y_test, combined_test_embeddings)\n\u001b[0;32m      6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_train_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Preparing Data and Training the Model\n",
    "# Assuming you have combined embeddings (X_train, X_test) and labels (y_train, y_test)\n",
    "train_dataset = HateSpeechDataset(X_train, y_train, combined_train_embeddings)\n",
    "test_dataset = HateSpeechDataset(X_test, y_test, combined_test_embeddings)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Model Parameters\n",
    "embedding_dim = 868  # Example: 100 from FastText + 768 from Bangla BERT\n",
    "hidden_dim = 128\n",
    "num_classes = len(set(y_train))  # Number of unique labels\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = HAN(embedding_dim, hidden_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shazzad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from gensim.models import FastText\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "bengali_stopwords = set(stopwords.words('bengali'))\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans text by removing unnecessary characters and symbols.\"\"\"\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep Bengali characters and whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text, tokenizer):\n",
    "    \"\"\"Tokenizes text using Bangla BERT tokenizer.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Removes Bengali stopwords from tokens.\"\"\"\n",
    "    return [token for token in tokens if token not in bengali_stopwords]\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"bangla_hate_speech.csv\"  # Replace with actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "df['cleaned_sentence'] = df['sentence'].apply(clean_text)\n",
    "\n",
    "# Load tokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "\n",
    "df['tokens'] = df['cleaned_sentence'].apply(lambda x: tokenize_text(x, bert_tokenizer))\n",
    "df['filtered_tokens'] = df['tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Split data\n",
    "X = df['filtered_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "y = df['hate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Ensure PyTorch is set up to use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Bangla BERT model onto GPU\n",
    "bert_model = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Hybrid Embeddings Generation\n",
    "# Train FastText\n",
    "def train_fasttext(corpus, embedding_dim=100):\n",
    "    model = FastText(sentences=corpus, vector_size=embedding_dim, window=5, min_count=1, sg=1)\n",
    "    return model\n",
    "\n",
    "# Prepare corpus for FastText\n",
    "corpus = df['filtered_tokens'].tolist()\n",
    "fasttext_model = train_fasttext(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def get_bert_embeddings(text):\n",
    "#     inputs = bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = bert_model(**inputs)\n",
    "#     return outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "# def combine_embeddings(tokens, fasttext_model, bert_embeddings, embedding_dim=100):\n",
    "#     combined_embeddings = []\n",
    "#     for idx, token in enumerate(tokens):\n",
    "#         fasttext_vec = fasttext_model.wv[token] if token in fasttext_model.wv else np.zeros(embedding_dim)\n",
    "#         bert_vec = bert_embeddings[idx].numpy() if idx < len(bert_embeddings) else np.zeros_like(bert_embeddings[0].numpy())\n",
    "#         combined_vec = np.concatenate((fasttext_vec, bert_vec))\n",
    "#         combined_embeddings.append(combined_vec)\n",
    "#     return np.array(combined_embeddings)\n",
    "\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    # Tokenize and move tensors to GPU\n",
    "    inputs = bert_tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    # Inference with no gradient to save memory and compute\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # outputs.last_hidden_state has shape [batch_size, seq_len, hidden_dim]\n",
    "    # If you're only handling a single text, you can squeeze the batch dimension\n",
    "    return outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "def combine_embeddings(tokens, fasttext_model, bert_embeddings, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tokens (list): List of tokens\n",
    "        fasttext_model: Gensim's FastText model\n",
    "        bert_embeddings (torch.Tensor): BERT embeddings on GPU\n",
    "        embedding_dim (int): Dimension of the FastText embeddings\n",
    "    Returns:\n",
    "        torch.Tensor of shape [len(tokens), embedding_dim + bert_hidden_size]\n",
    "    \"\"\"\n",
    "    combined_embeddings = []\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        # 1. Get FastText vector (on CPU), then move to GPU\n",
    "        if token in fasttext_model.wv:\n",
    "            fasttext_vec = fasttext_model.wv[token]\n",
    "        else:\n",
    "            fasttext_vec = np.zeros(embedding_dim)\n",
    "        fasttext_vec = torch.tensor(fasttext_vec, dtype=torch.float32, device=device)\n",
    "\n",
    "        # 2. Get the corresponding BERT embedding (already on GPU)\n",
    "        if idx < bert_embeddings.size(0):\n",
    "            bert_vec = bert_embeddings[idx]\n",
    "        else:\n",
    "            bert_vec = torch.zeros_like(bert_embeddings[0], device=device)\n",
    "\n",
    "        # 3. Concatenate FastText and BERT embedding along the last dimension\n",
    "        combined_vec = torch.cat((fasttext_vec, bert_vec), dim=0)\n",
    "        combined_embeddings.append(combined_vec)\n",
    "\n",
    "    # Convert list of tensors to a single tensor of shape [num_tokens, total_dim]\n",
    "    combined_embeddings = torch.stack(combined_embeddings)\n",
    "    return combined_embeddings\n",
    "\n",
    "# Usage example:\n",
    "# tokens = [\"hello\", \"world\", \"this\", \"is\", \"gpu\"]\n",
    "# text = \"hello world this is gpu\"\n",
    "# bert_emb = get_bert_embeddings(text)\n",
    "# combined = combine_embeddings(tokens, fasttext_model, bert_emb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_safe(text, fasttext_model):\n",
    "    \"\"\"\n",
    "    Returns a zero embedding if the text is empty\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        # For example, return a zero tensor with shape [1, your_dimension]\n",
    "        # or skip. This is up to your design.\n",
    "        return torch.zeros((1, 868), device=device)  # If your BERT hidden dim is 768\n",
    "    tokens = text.split()\n",
    "    bert_emb = get_bert_embeddings(\" \".join(tokens))  # get_bert_embeddings also on GPU\n",
    "    return combine_embeddings(tokens, fasttext_model, bert_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train embeddings: 100%|██████████| 24000/24000 [07:22<00:00, 54.25it/s] \n",
      "Generating test embeddings: 100%|██████████| 6000/6000 [01:54<00:00, 52.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "train_embeddings = [\n",
    "    get_embeddings_safe(tokens, fasttext_model) \n",
    "    for tokens in tqdm(X_train, desc=\"Generating train embeddings\")\n",
    "]\n",
    "test_embeddings = [\n",
    "    get_embeddings_safe(tokens, fasttext_model) \n",
    "    for tokens in tqdm(X_test, desc=\"Generating test embeddings\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Each item in 'batch' is a tuple: (embeddings, label)\n",
    "    - embeddings: shape [seq_len, embedding_dim]\n",
    "    - label: scalar\n",
    "    We'll pad embeddings so they all match the longest seq_len in this batch.\n",
    "    \"\"\"\n",
    "    embeddings_list = [item[0] for item in batch]  # list of [seq_len, embed_dim] tensors\n",
    "    labels_list = [item[1] for item in batch]      # list of label tensors\n",
    "\n",
    "    # Pad embeddings to [batch_size, max_seq_len_in_batch, embed_dim]\n",
    "    padded_embeddings = pad_sequence(embeddings_list, batch_first=True)\n",
    "\n",
    "    # Stack labels, shape [batch_size]\n",
    "    labels_tensor = torch.stack(labels_list)\n",
    "\n",
    "    return padded_embeddings, labels_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, emb in enumerate(train_embeddings):\n",
    "    if emb.shape[1] == 768:\n",
    "        print(i, emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Accuracy = 0.8293\n",
      "Epoch 2: Accuracy = 0.8355\n",
      "Epoch 3: Accuracy = 0.8642\n",
      "Epoch 4: Accuracy = 0.8655\n",
      "Epoch 5: Accuracy = 0.8683\n",
      "Epoch 6: Accuracy = 0.8688\n",
      "Epoch 7: Accuracy = 0.8613\n",
      "Epoch 8: Accuracy = 0.8678\n",
      "Epoch 9: Accuracy = 0.8662\n",
      "Epoch 10: Accuracy = 0.8717\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Hierarchical Attention Network (HAN)\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx].float(), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_classes):\n",
    "        super(HAN, self).__init__()\n",
    "        self.word_lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.word_attention = nn.Linear(2 * hidden_dim, 1)\n",
    "        self.sentence_lstm = nn.LSTM(2 * hidden_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.sentence_attention = nn.Linear(2 * hidden_dim, 1)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, num_classes)\n",
    "\n",
    "    def attention(self, lstm_output, attention_layer):\n",
    "        attention_weights = torch.softmax(attention_layer(lstm_output), dim=1)\n",
    "        weighted_output = torch.sum(attention_weights * lstm_output, dim=1)\n",
    "        return weighted_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        word_output, _ = self.word_lstm(x)\n",
    "        sentence_input = self.attention(word_output, self.word_attention)\n",
    "        sentence_output, _ = self.sentence_lstm(sentence_input.unsqueeze(1))\n",
    "        document_representation = self.attention(sentence_output, self.sentence_attention)\n",
    "        logits = self.fc(document_representation)\n",
    "        return logits\n",
    "\n",
    "train_dataset = HateSpeechDataset(train_embeddings, y_train.tolist())\n",
    "test_dataset = HateSpeechDataset(test_embeddings, y_test.tolist())\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "embedding_dim = 868  # 100 (FastText) + 768 (Bangla BERT)\n",
    "hidden_dim = 128\n",
    "num_classes = len(set(y_train))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = HAN(embedding_dim, hidden_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for embeddings, labels in train_loader:\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in test_loader:\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "            outputs = model(embeddings)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch + 1}: Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model, fasttext_model, bert_tokenizer, bert_model, device):\n",
    "    \"\"\"\n",
    "    Predict the label for a single piece of text using the trained HAN model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1) Preprocess text (tokenize, embed)\n",
    "    tokens = text.split()\n",
    "    # Embed using your known pipeline:\n",
    "    # e.g. get_bert_embeddings, then combine_embeddings\n",
    "    with torch.no_grad():\n",
    "        bert_emb = get_bert_embeddings(\" \".join(tokens))  # the function you used before\n",
    "        sample_emb = combine_embeddings(tokens, fasttext_model, bert_emb)  # shape [seq_len, 868]\n",
    "    \n",
    "    # 2) Model expects shape [batch_size, seq_len, embedding_dim]\n",
    "    sample_emb = sample_emb.unsqueeze(0).to(device)  # shape [1, seq_len, 868]\n",
    "    \n",
    "    # 3) Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sample_emb)\n",
    "        # outputs: [1, num_classes]\n",
    "\n",
    "    # 4) Predicted class\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for 'পিলখানা হত্যাকান্ড বাংলাদেশের প্রতিরক্ষা ব্যবস্থা ধ্বংসের জন্য ভারতের প্রত্যক্ষ সহযোগিতায় এই হত্যাকা- ঘটানো হয়েছিল': 1\n"
     ]
    }
   ],
   "source": [
    "# Example usage (Bangla text)\n",
    "example_text = \"পিলখানা হত্যাকান্ড বাংলাদেশের প্রতিরক্ষা ব্যবস্থা ধ্বংসের জন্য ভারতের প্রত্যক্ষ সহযোগিতায় এই হত্যাকা- ঘটানো হয়েছিল\"\n",
    "pred_label = predict_text(\n",
    "    text=example_text, \n",
    "    model=model, \n",
    "    fasttext_model=fasttext_model,\n",
    "    bert_tokenizer=bert_tokenizer,\n",
    "    bert_model=bert_model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Predicted label for '{example_text}': {pred_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
