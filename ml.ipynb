{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "Requirement already satisfied: fasttext-wheel in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.0)\n",
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp310-cp310-win_amd64.whl (203.1 MB)\n",
      "Collecting shap\n",
      "  Using cached shap-0.46.0-cp310-cp310-win_amd64.whl (456 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.0)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0\n",
      "  Using cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fasttext-wheel) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fasttext-wheel) (63.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Collecting sympy==1.13.1\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Collecting numba\n",
      "  Using cached numba-0.60.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "Collecting slicer==0.0.8\n",
      "  Using cached slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Using cached llvmlite-0.43.0-cp310-cp310-win_amd64.whl (28.1 MB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Installing collected packages: mpmath, sympy, slicer, safetensors, llvmlite, jinja2, fsspec, filelock, cloudpickle, torch, numba, nltk, huggingface-hub, tokenizers, shap, seaborn, transformers\n",
      "Successfully installed cloudpickle-3.1.0 filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.0 jinja2-3.1.5 llvmlite-0.43.0 mpmath-1.3.0 nltk-3.9.1 numba-0.60.0 safetensors-0.4.5 seaborn-0.13.2 shap-0.46.0 slicer-0.0.8 sympy-1.13.1 tokenizers-0.21.0 torch-2.5.1 transformers-4.47.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy nltk transformers fasttext-wheel scikit-learn torch shap matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download the Bengali stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load Bengali stopwords\n",
    "bengali_stopwords = set(stopwords.words('bengali'))\n",
    "\n",
    "# Initialize tokenizer for later use\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preview:\n",
      "                                            sentence  hate category\n",
      "0                     যত্তসব পাপন শালার ফাজলামী!!!!!     1   sports\n",
      "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার     1   sports\n",
      "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...     1   sports\n",
      "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়     1   sports\n",
      "4   তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব     1   sports\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Load the Dataset\n",
    "# Replace 'dataset.csv' with the path to your dataset file\n",
    "file_path = 'bangla_hate_speech.csv'  # Update this with your actual dataset path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to verify data loading\n",
    "print(\"Data Preview:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Cleaning:\n",
      "                                            sentence  \\\n",
      "0                     যত্তসব পাপন শালার ফাজলামী!!!!!   \n",
      "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার   \n",
      "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...   \n",
      "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়   \n",
      "4   তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব   \n",
      "\n",
      "                                    cleaned_sentence  \n",
      "0                          যত্তসব পাপন শালার ফাজলামী  \n",
      "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার  \n",
      "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...  \n",
      "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়  \n",
      "4    তুই তো শালা গাজা খাইছচতুর মার হেডায় খেলবে সাকিব  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 3: Text Cleaning\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing unnecessary characters and symbols.\n",
    "    \"\"\"\n",
    "    # Remove special characters, numbers, and symbols, keeping only Bengali letters\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep Bengali characters and whitespace\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'sentence' column\n",
    "df['cleaned_sentence'] = df['sentence'].apply(clean_text)\n",
    "\n",
    "print(\"After Cleaning:\")\n",
    "print(df[['sentence', 'cleaned_sentence']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Tokenization:\n",
      "                                    cleaned_sentence  \\\n",
      "0                          যত্তসব পাপন শালার ফাজলামী   \n",
      "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার   \n",
      "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...   \n",
      "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়   \n",
      "4    তুই তো শালা গাজা খাইছচতুর মার হেডায় খেলবে সাকিব   \n",
      "\n",
      "                                              tokens  \n",
      "0      [যত, ##ত, ##সব, পাপন, শালা, ##র, ফাজলাম, ##ী]  \n",
      "1    [পাপন, শালা, রে, রিমান, ##ডে, নেও, ##যা, দরকার]  \n",
      "2  [জিল, ##ল, ##র, রহমান, স, ##যার, ##ের, ছেলে, এ...  \n",
      "3  [শালা, ল, ##চ, ##চা, দেখতে, পাঠ, ##ার, মত, দেখ...  \n",
      "4  [ত, ##ই, তে, ##া, শালা, গাজা, খাই, ##ছ, ##চতর,...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 4: Tokenization with Bangla BERT\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text using Bangla BERT tokenizer.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into subwords for better context understanding\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Convert tokens to a single string for training input\n",
    "    return tokens\n",
    "\n",
    "# Tokenize cleaned sentences\n",
    "df['tokens'] = df['cleaned_sentence'].apply(tokenize_text)\n",
    "\n",
    "print(\"After Tokenization:\")\n",
    "print(df[['cleaned_sentence', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stopword Removal:\n",
      "                                              tokens  \\\n",
      "0      [যত, ##ত, ##সব, পাপন, শালা, ##র, ফাজলাম, ##ী]   \n",
      "1    [পাপন, শালা, রে, রিমান, ##ডে, নেও, ##যা, দরকার]   \n",
      "2  [জিল, ##ল, ##র, রহমান, স, ##যার, ##ের, ছেলে, এ...   \n",
      "3  [শালা, ল, ##চ, ##চা, দেখতে, পাঠ, ##ার, মত, দেখ...   \n",
      "4  [ত, ##ই, তে, ##া, শালা, গাজা, খাই, ##ছ, ##চতর,...   \n",
      "\n",
      "                                     filtered_tokens  \n",
      "0          [##ত, ##সব, পাপন, শালা, ##র, ফাজলাম, ##ী]  \n",
      "1    [পাপন, শালা, রে, রিমান, ##ডে, নেও, ##যা, দরকার]  \n",
      "2  [জিল, ##ল, ##র, রহমান, স, ##যার, ##ের, ছেলে, #...  \n",
      "3           [শালা, ল, ##চ, ##চা, পাঠ, ##ার, মত, ##য]  \n",
      "4  [ত, ##ই, তে, ##া, শালা, গাজা, খাই, ##ছ, ##চতর,...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 5: Stopword Removal\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Removes Bengali stopwords from the token list.\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token not in bengali_stopwords]\n",
    "\n",
    "# Apply stopword removal on tokens\n",
    "df['filtered_tokens'] = df['tokens'].apply(remove_stopwords)\n",
    "\n",
    "print(\"After Stopword Removal:\")\n",
    "print(df[['tokens', 'filtered_tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 24000\n",
      "Testing Data Size: 6000\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Prepare Data for Modeling\n",
    "# Map the sentences and their respective labels\n",
    "X = df['filtered_tokens'].apply(lambda tokens: ' '.join(tokens))  # Join tokens for input\n",
    "y = df['hate']  # Assuming the 'hate' column contains the labels\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Display dataset statistics\n",
    "print(\"Training Data Size:\", len(X_train))\n",
    "print(\"Testing Data Size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Data saved to disk.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Save Preprocessed Data\n",
    "# Save the train and test splits for later use\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing complete. Data saved to disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.7/61.7 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0\n",
      "  Downloading scipy-1.13.1-cp310-cp310-win_amd64.whl (46.2 MB)\n",
      "     -------------------------------------- 46.2/46.2 MB 889.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\~cipy.libs\\\\libscipy_openblas-5b1ec8b915dfb81d11cebc0788069d2d.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastText\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from gensim.models import FastText\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: FastText Embeddings\n",
    "def train_fasttext(corpus, embedding_dim=100, window_size=5, min_count=1):\n",
    "    \"\"\"\n",
    "    Train a FastText model on the dataset.\n",
    "    :param corpus: List of tokenized sentences.\n",
    "    :param embedding_dim: Dimension of the embeddings.\n",
    "    :param window_size: Context window size.\n",
    "    :param min_count: Minimum word count threshold.\n",
    "    :return: Trained FastText model.\n",
    "    \"\"\"\n",
    "    model = FastText(sentences=corpus, vector_size=embedding_dim, window=window_size, min_count=min_count, sg=1)\n",
    "    return model\n",
    "\n",
    "# Convert the filtered tokens to a list of tokenized sentences\n",
    "corpus = df['filtered_tokens'].tolist()\n",
    "\n",
    "# Train the FastText model\n",
    "fasttext_model = train_fasttext(corpus)\n",
    "print(\"FastText training complete!\")\n",
    "\n",
    "# Example: Retrieve FastText vector for a word\n",
    "word = \"বাংলাদেশ\"\n",
    "if word in fasttext_model.wv:\n",
    "    print(f\"FastText vector for '{word}': {fasttext_model.wv[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Bangla BERT Embeddings\n",
    "# Load Bangla BERT model and tokenizer\n",
    "bangla_bert_model = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "bangla_bert_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    \"\"\"\n",
    "    Extract BERT embeddings for the input text.\n",
    "    :param text: Input sentence.\n",
    "    :return: Token-level embeddings.\n",
    "    \"\"\"\n",
    "    inputs = bangla_bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bangla_bert_model(**inputs)\n",
    "    # Get the last hidden state (token embeddings)\n",
    "    return outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "# Example: Get Bangla BERT embeddings for a sentence\n",
    "sentence = \"বাংলাদেশ একটি সুন্দর দেশ।\"\n",
    "bert_embeddings = get_bert_embeddings(sentence)\n",
    "print(f\"BERT Embeddings shape: {bert_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 3: Combine FastText and BERT Embeddings\n",
    "def combine_embeddings(tokens, fasttext_model, bert_embeddings, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Combine FastText and BERT embeddings for each token.\n",
    "    :param tokens: List of tokens for the sentence.\n",
    "    :param fasttext_model: Trained FastText model.\n",
    "    :param bert_embeddings: BERT embeddings for the tokens.\n",
    "    :param embedding_dim: Dimension of FastText embeddings.\n",
    "    :return: Combined embeddings for each token.\n",
    "    \"\"\"\n",
    "    combined_embeddings = []\n",
    "    for idx, token in enumerate(tokens):\n",
    "        # Get FastText embedding (zeros if not in vocab)\n",
    "        fasttext_vec = fasttext_model.wv[token] if token in fasttext_model.wv else np.zeros(embedding_dim)\n",
    "        \n",
    "        # Get BERT embedding for the token\n",
    "        bert_vec = bert_embeddings[idx].numpy() if idx < len(bert_embeddings) else np.zeros_like(bert_embeddings[0].numpy())\n",
    "        \n",
    "        # Concatenate FastText and BERT embeddings\n",
    "        combined_vec = np.concatenate((fasttext_vec, bert_vec))\n",
    "        combined_embeddings.append(combined_vec)\n",
    "    \n",
    "    return np.array(combined_embeddings)\n",
    "\n",
    "# Example: Combine embeddings for a sentence\n",
    "tokens = df['filtered_tokens'][0]  # Use the first sentence in the dataset\n",
    "bert_embs = get_bert_embeddings(\" \".join(tokens))\n",
    "combined_embs = combine_embeddings(tokens, fasttext_model, bert_embs)\n",
    "print(f\"Combined Embeddings shape: {combined_embs.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
